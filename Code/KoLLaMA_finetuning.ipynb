{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f45f65c1a6294793bd8dd94a066011f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5512124d46c34f46aeb55a276eba375c",
              "IPY_MODEL_31c5a32a85c440c89a9eab60d179117c",
              "IPY_MODEL_edc041e0271341ffb3dcf0ad1fd8fe67"
            ],
            "layout": "IPY_MODEL_bfd0ad7202594c69a359f5c55182e3f2"
          }
        },
        "5512124d46c34f46aeb55a276eba375c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b8ec6692fa0405089fe8d3d17fd559a",
            "placeholder": "​",
            "style": "IPY_MODEL_a624fcb046fa4a838b2cb18ff254cf42",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "31c5a32a85c440c89a9eab60d179117c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91123e4e2d540c7a75230d78ff10383",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2096d3186c034cbe8c484686925893df",
            "value": 4
          }
        },
        "edc041e0271341ffb3dcf0ad1fd8fe67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ebb58e0ec374141a1d5b5e49e4361b6",
            "placeholder": "​",
            "style": "IPY_MODEL_b73b9ace2cea463cb66e410adf6a22da",
            "value": " 4/4 [01:32&lt;00:00, 20.26s/it]"
          }
        },
        "bfd0ad7202594c69a359f5c55182e3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8ec6692fa0405089fe8d3d17fd559a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a624fcb046fa4a838b2cb18ff254cf42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b91123e4e2d540c7a75230d78ff10383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2096d3186c034cbe8c484686925893df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ebb58e0ec374141a1d5b5e49e4361b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b73b9ace2cea463cb66e410adf6a22da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da2197600d1845c59048fcc1e337c3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62acb151b7944594b4dc542609ece2dd",
              "IPY_MODEL_56bd084d6a194529a4ab7d544bc5ce59",
              "IPY_MODEL_e7c13ccaf40847f7b77e76fde4dfb048"
            ],
            "layout": "IPY_MODEL_1722fd2ac44c4c8ab020799d3cc91f6d"
          }
        },
        "62acb151b7944594b4dc542609ece2dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e65dcf4748584369adcdbca71e214a0b",
            "placeholder": "​",
            "style": "IPY_MODEL_be07429f46164fa882a99e523f40b31f",
            "value": "Map: 100%"
          }
        },
        "56bd084d6a194529a4ab7d544bc5ce59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316d21c97c7b42dd86a1a5510476d39c",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_764a6646cd8d4e5d93656327e7f6fb06",
            "value": 23
          }
        },
        "e7c13ccaf40847f7b77e76fde4dfb048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d142995aef2e4a41ad5307436c90f66a",
            "placeholder": "​",
            "style": "IPY_MODEL_aa9dfac3d9ce420c83ca6f940bb95a58",
            "value": " 23/23 [00:00&lt;00:00, 236.94 examples/s]"
          }
        },
        "1722fd2ac44c4c8ab020799d3cc91f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65dcf4748584369adcdbca71e214a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be07429f46164fa882a99e523f40b31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "316d21c97c7b42dd86a1a5510476d39c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "764a6646cd8d4e5d93656327e7f6fb06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d142995aef2e4a41ad5307436c90f66a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9dfac3d9ce420c83ca6f940bb95a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e688c28dc6354e01ba96bd94238d5941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61fb0f0b489d43619c3e67a74f35ce6a",
              "IPY_MODEL_c301cc49a9ce4873814fa7199999d329",
              "IPY_MODEL_50d35994dc76402fb757bd844a156724"
            ],
            "layout": "IPY_MODEL_6b575398d1e247a1879fb2eedda059ff"
          }
        },
        "61fb0f0b489d43619c3e67a74f35ce6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_279d2332163b4f6f95dc1ed3c867254c",
            "placeholder": "​",
            "style": "IPY_MODEL_a5bc3e17ad51430da785bb4dabeded09",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c301cc49a9ce4873814fa7199999d329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc30dee77ca47e9bb13b1dd24d1c458",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0bbb4e913884cc29bfaa7c3f9d27d49",
            "value": 4
          }
        },
        "50d35994dc76402fb757bd844a156724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c57e4231b4e84ddeb498e1648a607925",
            "placeholder": "​",
            "style": "IPY_MODEL_d721568fd1b9412298e56ce35b9b4deb",
            "value": " 4/4 [01:34&lt;00:00, 20.90s/it]"
          }
        },
        "6b575398d1e247a1879fb2eedda059ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279d2332163b4f6f95dc1ed3c867254c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5bc3e17ad51430da785bb4dabeded09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dc30dee77ca47e9bb13b1dd24d1c458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0bbb4e913884cc29bfaa7c3f9d27d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c57e4231b4e84ddeb498e1648a607925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d721568fd1b9412298e56ce35b9b4deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "-9yYUQMaBNSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install trl==0.12.2"
      ],
      "metadata": {
        "id": "v4oX1yNHBNQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "FhW7xCUIBNOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U transformers"
      ],
      "metadata": {
        "id": "Ho8SXjx_BNMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_community"
      ],
      "metadata": {
        "id": "twzIxrEABNKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain_experimental"
      ],
      "metadata": {
        "id": "jjU1iYlkBNIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index"
      ],
      "metadata": {
        "id": "1ZDt8Q2D4flD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index.embeddings.huggingface"
      ],
      "metadata": {
        "id": "4ennsMb_5N68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index.llms.huggingface"
      ],
      "metadata": {
        "id": "Tih1mVF47C3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "L2ISoJ3JA9EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA 모델 로드 순서\n",
        "# 1. 기본 모델을 4-bit 양자화하여 로드\n",
        "# 2. LoRA 어댑터 (기존 LoRA 모델) 를 추가하여 QLoRA 모델로 변환"
      ],
      "metadata": {
        "id": "IuoL_GuSOjAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit 양자화 설정\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "YcoDlRY6OjsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660,
          "referenced_widgets": [
            "f45f65c1a6294793bd8dd94a066011f0",
            "5512124d46c34f46aeb55a276eba375c",
            "31c5a32a85c440c89a9eab60d179117c",
            "edc041e0271341ffb3dcf0ad1fd8fe67",
            "bfd0ad7202594c69a359f5c55182e3f2",
            "1b8ec6692fa0405089fe8d3d17fd559a",
            "a624fcb046fa4a838b2cb18ff254cf42",
            "b91123e4e2d540c7a75230d78ff10383",
            "2096d3186c034cbe8c484686925893df",
            "2ebb58e0ec374141a1d5b5e49e4361b6",
            "b73b9ace2cea463cb66e410adf6a22da"
          ]
        },
        "id": "_h4Zzs7OArDs",
        "outputId": "f29c3cf4-9e7e-44b1-ddb2-bb9f23e9cb24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f45f65c1a6294793bd8dd94a066011f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# 4-bit 양자화된 기본 모델 로드\n",
        "base_model_name = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'  # Hugging Face 모델 or 로컬 경로\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
        "tokenizer.padding_side = 'right'  # 패딩 방향 설정\n",
        "\n",
        "# 전체 모델을 GPU로 강제 이동\n",
        "base_model.to(\"cuda\")\n",
        "\n",
        "base_model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 하이퍼 파라미터\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/kollama\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2, # 각 GPU당 배치 사이즈 - 기본값은 8\n",
        "    gradient_accumulation_steps = 1, # 기울기 축적 단계 - 기본값은 1, 커질수록 정확도가 커짐, 곱하기 개념.\n",
        "                                     # 값을 늘리면 처리 속도는 빨라지지만 GPU 사용량이 커짐\n",
        "    gradient_checkpointing = True, # 기본값은 False이며 True로 설정하면 역 방향 패스 시 메모리를 절약함\n",
        "    max_grad_norm = 1.0, # 최대 기울기 표준 - 기본값은 1.0이며 gradient clipping을 위한 변수\n",
        "                         # Gradient Clipping : Gradient Explosion 문제를 방지하기 위해 기울기의 크기를 특정 임계값 이하로 제한하는 기법\n",
        "    num_train_epochs = 8, # 훈련할 총 에포크 수\n",
        "    learning_rate = 1e-5, # 학습률\n",
        "    bf16 = True, # 32비트 훈련 대신 bf16비트 혼합된 정밀도 훈련을 사용할 지 여부\n",
        "    save_total_limit = 3, # 설정 시, 오래된 체크포인트 순으로 삭제함\n",
        "    logging_steps = 10, # 로깅하는 단위 (logging=10 이면 10번마다 로깅)\n",
        "    output_dir = output_dir, # 결과 경로\n",
        "    optim = \"paged_adamw_32bit\", # 사용하려는 옵티마이저 종류 (adam_hf, adamw_torch, adamw_apex_fused, adafactor 등이 있음)\n",
        "    lr_scheduler_type = \"cosine\", # 사용하는 스케줄러 타입 - 기본값은 linear, 종류는 cosine 등이 있음\n",
        "    warmup_ratio = 0.05, # 웜업 비율 - 기본값은 0\n",
        "    max_steps = -1, # 수행할 총 훈련 단계 수 - 기본값은 -1\n",
        "                    # overrides_num_train_epoch 설정한 스텝 도달 전에 돌릴 수 있는 모든 데이터가 소진되면 멈춤\n",
        "                    # 데이터셋 크기가 작기 때문에 너무 많은 스텝을 돌리면 기존 모델의 지식을 과도하게 덮어쓸 수도 있음\n",
        "    report_to = 'tensorboard', # 결과와 로그를 리포트 저장하는 장소 - all이 기본값이며 all로 설정하는 경우\n",
        "                               # 연결된 모든 리포트 도구에 저장함 (azure_ml, comet_ml, mlflow, tensorboard, wandb 등이 있음)\n",
        "    remove_unused_columns = False,  # 중요 입력 데이터 삭제 방지\n",
        "    weight_decay = 0.05 # 규제\n",
        ")"
      ],
      "metadata": {
        "id": "JZGdyM6mCC82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#JSON 파일로드\n",
        "with open(r\"/content/drive/MyDrive/Colab Notebooks/scenario/txt/finetuning_dataset2.json\", 'r', encoding = \"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# 토큰화 함수 정의\n",
        "def tokenize_function(example):\n",
        "    # 전체 대화 내용을 하나의 문자열로 결합\n",
        "    full_conversation = \"\"\n",
        "    for message in example['messages']:\n",
        "        role = message['role']\n",
        "        content = message['content']\n",
        "        full_conversation += f\"<|{role}|>: {content}\\n\"  # <|role|>: 내용 형식으로 변환\n",
        "\n",
        "    # 토크나이즈\n",
        "    tokenized = tokenizer(\n",
        "        full_conversation,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    # labels 생성 (입력과 동일하게)\n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# 리스트를 Dataset으로 변환\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "\n",
        "# 데이터셋에 토큰화 함수 적용\n",
        "tokenized_dataset = dataset.map(tokenize_function, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "da2197600d1845c59048fcc1e337c3ef",
            "62acb151b7944594b4dc542609ece2dd",
            "56bd084d6a194529a4ab7d544bc5ce59",
            "e7c13ccaf40847f7b77e76fde4dfb048",
            "1722fd2ac44c4c8ab020799d3cc91f6d",
            "e65dcf4748584369adcdbca71e214a0b",
            "be07429f46164fa882a99e523f40b31f",
            "316d21c97c7b42dd86a1a5510476d39c",
            "764a6646cd8d4e5d93656327e7f6fb06",
            "d142995aef2e4a41ad5307436c90f66a",
            "aa9dfac3d9ce420c83ca6f940bb95a58"
          ]
        },
        "id": "DjEFuOqAqsDw",
        "outputId": "d431461f-42b9-4747-e3f9-24e57e538304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da2197600d1845c59048fcc1e337c3ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZbek1-iq8qw",
        "outputId": "bd505cf9-35d0-4458-a9fc-4e014ee807a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 23\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# #JSON 파일로드\n",
        "# with open(r\"/content/drive/MyDrive/Colab Notebooks/scenario/txt/finetuning_dataset.json\", 'r', encoding = \"utf-8\") as f:\n",
        "#     raw_data = json.load(f)\n",
        "\n",
        "# # 토큰화 함수 정의\n",
        "# def tokenize_function(example):\n",
        "#     # 전체 대화 내용을 하나의 문자열로 결합\n",
        "#     full_conversation = \"\"\n",
        "#     for message in example['messages']:\n",
        "#         role = message['role']\n",
        "#         content = message['content']\n",
        "#         full_conversation += f\"<|{role}|>: {content}\\n\"\n",
        "\n",
        "#     # 토크나이즈\n",
        "#     tokenized = tokenizer(\n",
        "#         full_conversation,\n",
        "#         padding='max_length',\n",
        "#         truncation=True,\n",
        "#         max_length=512,\n",
        "#     )\n",
        "\n",
        "#     # labels 생성\n",
        "#     tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "\n",
        "#     return tokenized\n",
        "\n",
        "# # 리스트를 Dataset으로 변환\n",
        "# dataset = Dataset.from_list(raw_data)\n",
        "\n",
        "# # 데이터셋에 토큰화 함수 적용\n",
        "# tokenized_dataset = dataset.map(tokenize_function, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "Jb1SdzNBzoit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lora 어댑터 추가\n",
        "lora_config = LoraConfig(\n",
        "    r=4,  # rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # 인과 언어 모델링 - 문맥 기반 생성, 대화형 생성, 연속적인 텍스트 생성\n",
        ")"
      ],
      "metadata": {
        "id": "pqEBErtaEF_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델에 Lora 어댑터 적용\n",
        "model = get_peft_model(base_model, lora_config)"
      ],
      "metadata": {
        "id": "YF8VvwYrEIN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "id": "iV5o4IkdIUiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = tokenized_dataset,\n",
        "    dataset_text_field=\"input_ids\",\n",
        "    tokenizer = tokenizer,\n",
        "    max_seq_length = 512,\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "output_dir = os.path.join(output_dir, \"KoLLaMA_checkpoint\")\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "EbwHcGM9EJP2",
        "outputId": "ead23d14-0262-4e6a-9dd0-609842231e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 36:00, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.815400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.844100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.664100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.612100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.515900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.429900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.449500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/kollama/KoLLaMA_checkpoint/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/kollama/KoLLaMA_checkpoint/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/kollama/KoLLaMA_checkpoint/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Yhc0YlEOj-",
        "outputId": "9629512c-cca1-4ca9-eb52-e0db749bb0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT = '''당신은 항상 보이스피싱 범죄자 역할을 하는 AI 튜터입니다. 절대 피해자 역할을 하지 않습니다.\n",
        "#     당신의 역할은 보이스피싱 사기에 사용될 수 있는 다양한 상황 중 하나를 생성하고 상대방과 대화를 나누며, 상대방을 속이려 노력하세요.\n",
        "\n",
        "#     **보이스피싱 시뮬레이션을 진행하는 방법**\n",
        "#     - 먼저 **랜덤한 보이스피싱 시나리오**를 하나 선택해서 사용자에게 제시해.\n",
        "#     - 사용자가 대응할 수 있도록 **실감 나게 연기**해.\n",
        "#     - 사용자가 대화 종료 라고 입력하면 대화를 종료하고 **올바른 대응인지 피드백을 제공**해.\n",
        "#     - 피드백을 제공할 때는 상세하게 제공해주고, 사용자의 대응과 별개로 올바른 대응이 어떤건지 알려줘.\n",
        "#     - 대화가 종료되기 전까지는 보이스피싱 가해자처럼 계속 사용자를 속이려고 노력해야해.\n",
        "#     - 대화에는 너의 질문과 나의 답변 외에는 출력하지 않도록 해.\n",
        "#     - 사용자가 올바르게 대응하면 칭찬하고, 부족하면 어떻게 대응해야 하는지 알려줘.\n",
        "#     - 문장은 대부분 1문장에서 2문장씩만 이야기 해.\n",
        "\n",
        "#     **보이스피싱 시나리오 예시**\n",
        "#     - 경찰 사칭: \"고객님의 계좌에서 불법 거래가 감지되었습니다.\"\n",
        "#     - 은행 사칭: \"대출 승인이 완료되었으니 계좌 정보를 입력해주세요.\"\n",
        "#     - 대출 사기: \"신용등급을 올리려면 보증금이 필요합니다.\"\n",
        "#     - 가족 납치: \"아드님이 납치되었습니다. 돈을 입금하세요.\"\n",
        "#     - 협박: \"당신의 개인정보를 해킹했습니다. 돈을 보내지 않으면 유출하겠습니다.\"\n",
        "\n",
        "#     **목표: 사용자가 보이스피싱을 잘 구별하고, 올바르게 대응하도록 돕기**'''\n",
        "\n",
        "# # 대화 이력을 저장하는 리스트. 최초 시스템 메시지만 담아 시작합니다.\n",
        "# conversation_history = [\n",
        "#     {\"role\": \"system\", \"content\": PROMPT}\n",
        "# ]\n",
        "\n",
        "# terminators = [\n",
        "#     tokenizer.eos_token_id,\n",
        "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]\n",
        "\n",
        "# print(\"대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\\n\")\n",
        "\n",
        "# while True:\n",
        "#     # 사용자 입력 받기\n",
        "#     user_input = input(\"User: \")\n",
        "\n",
        "#     if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "#         break\n",
        "\n",
        "#     # 사용자 메시지를 대화 이력에 추가\n",
        "#     conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "#     input_ids = tokenizer.apply_chat_template(\n",
        "#         conversation_history,\n",
        "#         add_generation_prompt=True,\n",
        "#         return_tensors=\"pt\"\n",
        "#     ).to(model.device)\n",
        "\n",
        "#     # 모델로부터 응답 생성\n",
        "#     outputs = model.generate(\n",
        "#         input_ids,\n",
        "#         max_new_tokens=2048,\n",
        "#         eos_token_id=terminators,\n",
        "#         do_sample=True,\n",
        "#         temperature=0.6,\n",
        "#         top_p=0.9\n",
        "#     )\n",
        "\n",
        "#     # 대화 이력의 길이만큼을 건너뛰고 새로 생성된 텍스트만 추출\n",
        "#     generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "#     print(\"Assistant:\", generated_text, \"\\n\")\n",
        "\n",
        "#     # 모델의 응답을 대화 이력에 추가하여, 이후의 대화에 참고되도록 함\n",
        "#     conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})"
      ],
      "metadata": {
        "id": "VDiTqVZwK3G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG***"
      ],
      "metadata": {
        "id": "TbUZQcIA3IgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "# from llama_index.core.settings import Settings\n",
        "\n",
        "# # \"Private-Data\" 폴더 내 PDF 문서 로드\n",
        "# resume = SimpleDirectoryReader(\"/content/drive/MyDrive/Colab Notebooks/rag/ragdata\").load_data()\n",
        "\n",
        "# # 트리 인덱스(TreeIndex) 생성\n",
        "# embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# new_index =VectorStoreIndex.from_documents(resume, embed_model=embed_model)\n",
        "\n",
        "# # # Hugging Face 기반 LLM (KoLLaMA)\n",
        "# # llm = HuggingFaceLLM(model_name='MLP-KTLim/llama-3-Korean-Bllossom-8B')\n",
        "\n",
        "# # # OpenAI의 GPT 대신 Hugging Face LLM(KoLLaMA) 사용\n",
        "# # Settings.llm = llm\n",
        "\n",
        "# # OpenAI API 사용 안 함\n",
        "# Settings.llm = None"
      ],
      "metadata": {
        "id": "nXM4nfUyLFzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 쿼리 엔진 생성\n",
        "# query_engine = new_index.as_query_engine()\n",
        "\n",
        "# response_scenario = query_engine.query(\"보이스피싱 시뮬레이션에 사용할 내용을 제공해줘.\")\n",
        "# response_strategy = query_engine.query(\"보이스피싱에 대한 올바른 대응 방법을 설명해줘.\")\n",
        "# print(response_scenario)\n",
        "# print(response_strategy)\n",
        "\n",
        "# # 🔥 응답이 나온 문서의 출처 확인\n",
        "# print(response_scenario.source_nodes)  # 문서 출처를 출력\n",
        "# print(response_strategy.source_nodes)"
      ],
      "metadata": {
        "id": "kwEGHAkH3lfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 없이 벡터를 저장해서 불러올 경우에는 아래 코드 사용"
      ],
      "metadata": {
        "id": "qMMv-WWT9Uw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_index.storage_context.persist()"
      ],
      "metadata": {
        "id": "2ZiIrvcP3ldK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# 저장된 인덱스를 다시 로드\n",
        "# storage_context = StorageContext.from_defaults(persist_dir=\"/content/drive/MyDrive/Colab Notebooks/rag/storage\")\n",
        "# new_index = load_index_from_storage(storage_context)"
      ],
      "metadata": {
        "id": "pdO23AXC3lay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # RAG를 이용해 받은 response.response 값\n",
        "# method_scenario = response_scenario.response\n",
        "# method_strategy = response_strategy.response\n",
        "\n",
        "# PROMPT = '''당신은 항상 보이스피싱 범죄자 역할을 하는 AI 튜터입니다. 절대 피해자 역할을 하지 않습니다.\n",
        "#     당신의 역할은 보이스피싱 사기에 사용될 수 있는 다양한 상황 중 하나를 생성하고 상대방과 대화를 나누며, 상대방을 속이려 노력하세요.\n",
        "\n",
        "#     **보이스피싱 시뮬레이션을 진행하는 방법**\n",
        "#     - 먼저 **랜덤한 보이스피싱 시나리오**를 하나 선택해서 사용자에게 제시해.\n",
        "#     - 사용자가 대응할 수 있도록 **실감 나게 연기**해.\n",
        "#     - 사용자가 대화 종료 라고 입력하면 대화를 종료하고 **올바른 대응인지 피드백을 제공**해.\n",
        "#     - 피드백을 제공할 때 우선 사용자의 대응이 적절했는지, 부족했는지 평가해줘.\n",
        "#     - 피드백을 제공할 때는 사용자의 대응에 대해 상세하게 제공해주고, 사용자의 대응과 별개로 올바른 대응이 어떤건지 알려줘.\n",
        "#     - 보이스피싱 가해자처럼 계속 사용자를 속이려고 노력해야해.\n",
        "#     - 더 이상 속일 수 없다고 판단되면 대화를 종료해줘.\n",
        "#     - 대화에는 너의 질문과 나의 답변 외에는 출력하지 않도록 해.\n",
        "#     - 문장은 대부분 1문장에서 2문장씩만 이야기 해.\n",
        "\n",
        "#     **보이스피싱 시나리오 예시**\n",
        "#     - 경찰 사칭: \"고객님의 계좌에서 불법 거래가 감지되었습니다.\"\n",
        "#     - 은행 사칭: \"대출 승인이 완료되었으니 계좌 정보를 입력해주세요.\"\n",
        "#     - 대출 사기: \"신용등급을 올리려면 보증금이 필요합니다.\"\n",
        "#     - 가족 납치: \"아드님이 납치되었습니다. 돈을 입금하세요.\"\n",
        "#     - 협박: \"당신의 개인정보를 해킹했습니다. 돈을 보내지 않으면 유출하겠습니다.\"\n",
        "\n",
        "#     **보이스피싱 시나리오 추가 참고 정보**\n",
        "#     {method_scenario}\n",
        "\n",
        "#     **보이스피싱 올바른 대응방법 추가 참고 정보**\n",
        "#     {method_strategy}\n",
        "\n",
        "#     **목표: 사용자가 보이스피싱을 잘 구별하고, 올바르게 대응하도록 돕기**'''\n",
        "\n",
        "# # 대화 이력을 저장하는 리스트. 최초 시스템 메시지만 담아 시작합니다.\n",
        "# conversation_history = [\n",
        "#     {\"role\": \"system\", \"content\": PROMPT}\n",
        "# ]\n",
        "\n",
        "# terminators = [\n",
        "#     tokenizer.eos_token_id,\n",
        "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]\n",
        "\n",
        "# print(\"대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\\n\")\n",
        "\n",
        "# while True:\n",
        "#     # 사용자 입력 받기\n",
        "#     user_input = input(\"User: \")\n",
        "\n",
        "#     if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "#         break\n",
        "\n",
        "#     # 사용자 메시지를 대화 이력에 추가\n",
        "#     conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "#     input_ids = tokenizer.apply_chat_template(\n",
        "#         conversation_history,\n",
        "#         add_generation_prompt=True,\n",
        "#         return_tensors=\"pt\"\n",
        "#     ).to(model.device)\n",
        "\n",
        "#     # 모델로부터 응답 생성\n",
        "#     outputs = model.generate(\n",
        "#         input_ids,\n",
        "#         max_new_tokens=2048,\n",
        "#         eos_token_id=terminators,\n",
        "#         do_sample=True,\n",
        "#         temperature=0.6,\n",
        "#         top_p=0.9\n",
        "#     )\n",
        "\n",
        "#     # 대화 이력의 길이만큼을 건너뛰고 새로 생성된 텍스트만 추출\n",
        "#     generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "#     print(\"Assistant:\", generated_text, \"\\n\")\n",
        "\n",
        "#     # 모델의 응답을 대화 이력에 추가하여, 이후의 대화에 참고되도록 함\n",
        "#     conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})"
      ],
      "metadata": {
        "id": "E2oRwf8D3lVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # RAG를 이용해 받은 response.response 값\n",
        "# method_scenario = response_scenario.response\n",
        "# method_strategy = response_strategy.response\n",
        "\n",
        "# PROMPT = '''당신은 항상 보이스피싱 범죄자 역할을 하는 AI 튜터입니다. 절대 피해자 역할을 하지 않습니다.\n",
        "#     당신의 역할은 보이스피싱 사기에 사용될 수 있는 다양한 상황 중 하나를 생성하고 상대방과 대화를 나누며, 상대방을 속이려 노력하세요.\n",
        "\n",
        "#     **보이스피싱 시뮬레이션을 진행하는 방법**\n",
        "#     - 먼저 **랜덤한 보이스피싱 시나리오**를 하나 선택해서 사용자에게 제시해.\n",
        "#     - 사용자가 대응할 수 있도록 **실감 나게 연기**해.\n",
        "#     - 사용자가 대화 종료 라고 입력하면 대화를 종료하고 **올바른 대응인지 피드백을 제공**해.\n",
        "#     - 피드백을 제공할 때 우선 사용자의 대응이 적절했는지, 부족했는지 평가해줘.\n",
        "#     - 피드백을 제공할 때는 사용자의 대응에 대해 상세하게 제공해주고, 사용자의 대응과 별개로 올바른 대응이 어떤건지 알려줘.\n",
        "#     - 보이스피싱 가해자처럼 계속 사용자를 속이려고 노력해야해.\n",
        "#     - 대화에는 너의 질문과 나의 답변 외에는 출력하지 않도록 해.\n",
        "#     - 문장은 대부분 1문장에서 2문장씩만 이야기 해.\n",
        "#     - **대화가 종료될 조건**을 자동으로 판단하여 종료합니다.\n",
        "\n",
        "#     **대화 종료 조건**:\n",
        "#     - 사용자가 보이스피싱에 적절히 대응한 경우 또는 대화가 더 이상 진행될 필요가 없다고 판단되면, **자연스럽게 대화를 종료**하고 피드백을 제공합니다.\n",
        "#     - \"대화 종료\"라는 입력 없이도, 모델이 스스로 대화를 종료하고 피드백을 제공할 수 있도록 합니다.\n",
        "\n",
        "#     **보이스피싱 시나리오 예시**\n",
        "#     - 경찰 사칭: \"고객님의 계좌에서 불법 거래가 감지되었습니다.\"\n",
        "#     - 은행 사칭: \"대출 승인이 완료되었으니 계좌 정보를 입력해주세요.\"\n",
        "#     - 대출 사기: \"신용등급을 올리려면 보증금이 필요합니다.\"\n",
        "#     - 가족 납치: \"아드님이 납치되었습니다. 돈을 입금하세요.\"\n",
        "#     - 협박: \"당신의 개인정보를 해킹했습니다. 돈을 보내지 않으면 유출하겠습니다.\"\n",
        "\n",
        "#     **보이스피싱 시나리오 추가 참고 정보**\n",
        "#     {method_scenario}\n",
        "\n",
        "#     **보이스피싱 올바른 대응방법 추가 참고 정보**\n",
        "#     {method_strategy}\n",
        "\n",
        "#     **목표: 사용자가 보이스피싱을 잘 구별하고, 올바르게 대응하도록 돕기**'''\n",
        "\n",
        "# # 대화 이력을 저장하는 리스트. 최초 시스템 메시지만 담아 시작합니다.\n",
        "# conversation_history = [\n",
        "#     {\"role\": \"system\", \"content\": PROMPT}\n",
        "# ]\n",
        "\n",
        "# terminators = [\n",
        "#     tokenizer.eos_token_id,\n",
        "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "# ]\n",
        "\n",
        "# print(\"대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\\n\")\n",
        "\n",
        "# while True:\n",
        "#     # 사용자 입력 받기\n",
        "#     user_input = input(\"User: \")\n",
        "\n",
        "#     if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "#         break\n",
        "\n",
        "#     # 사용자 메시지를 대화 이력에 추가\n",
        "#     conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "#     input_ids = tokenizer.apply_chat_template(\n",
        "#         conversation_history,\n",
        "#         add_generation_prompt=True,\n",
        "#         return_tensors=\"pt\"\n",
        "#     ).to(model.device)\n",
        "\n",
        "#     # 모델로부터 응답 생성\n",
        "#     outputs = model.generate(\n",
        "#         input_ids,\n",
        "#         max_new_tokens=2048,\n",
        "#         eos_token_id=terminators,\n",
        "#         do_sample=True,\n",
        "#         temperature=0.6,\n",
        "#         top_p=0.9\n",
        "#     )\n",
        "\n",
        "#     # 대화 이력의 길이만큼을 건너뛰고 새로 생성된 텍스트만 추출\n",
        "#     generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "#     print(\"Assistant:\", generated_text, \"\\n\")\n",
        "\n",
        "#     # 모델의 응답을 대화 이력에 추가하여, 이후의 대화에 참고되도록 함\n",
        "#     conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})"
      ],
      "metadata": {
        "id": "O3q9I5Ho_9C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=============================================================================================================**"
      ],
      "metadata": {
        "id": "LIK7WfIh18ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "# \"Private-Data\" 폴더 내 PDF 문서 로드\n",
        "resume = SimpleDirectoryReader(\"/content/drive/MyDrive/Colab Notebooks/rag/ragdata\").load_data()\n",
        "\n",
        "# 트리 인덱스(TreeIndex) 생성\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "new_index =VectorStoreIndex.from_documents(resume, embed_model=embed_model)\n",
        "\n",
        "# OpenAI API 사용 안 함\n",
        "Settings.llm = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQjhuqKDtloQ",
        "outputId": "452f3458-ebc9-4708-961e-5ca1e29de6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리 엔진 생성\n",
        "query_engine = new_index.as_query_engine()\n",
        "\n",
        "response_scenario = query_engine.query(\"보이스피싱 시뮬레이션에 사용할 내용을 제공해줘.\")\n",
        "response_strategy = query_engine.query(\"보이스피싱에 대한 올바른 대응 방법을 설명해줘.\")"
      ],
      "metadata": {
        "id": "97r481PktmR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG를 이용해 받은 response.response 값\n",
        "method_scenario = response_scenario.response\n",
        "method_strategy = response_strategy.response"
      ],
      "metadata": {
        "id": "37Eo6DxIfYl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = '''당신은 보이스피싱 범죄자를 연기하는 AI입니다. 당신의 목표는 사용자가 보이스피싱 상황을 경험하고, 올바른 대응법을 학습하도록 돕는 것입니다. 대화 중에는 절대로 설명이나 피드백을 주지 말고, 오직 범죄자 역할에 충실한 응답만 제공하세요.\n",
        "\n",
        "규칙:\n",
        "1. 응답은 반드시 한국어로만 작성합니다.\n",
        "2. 사용자의 입력에 대해 실제 범죄자가 하는 것처럼, **현실감 있는 응답**을 작성하세요.\n",
        "3. 단순히 \"경찰 사칭 시나리오입니다\" 같은 문구를 반복하지 말고, **상황에 맞는 구체적인 사기범 응답**을 하세요.\n",
        "4. 문장은 **1~2문장으로 간결하게 작성**하며, 상대방을 설득하는 어조를 유지하세요.\n",
        "5. 사용자가 대화 종료 라고 입력하면 대화를 종료하고 **올바른 대응인지 피드백을 제공**하세요.\n",
        "6. 피드백을 제공할 때는 **사용자의 대응에 대해 상세하게 평가**해주고, 사용자의 대응과 별개로 **올바른 대응이 어떤건지 알려**주세요.\n",
        "\n",
        "예시 시나리오:\n",
        "- 경찰 사칭: \"고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 지금 즉시 주민등록번호와 계좌번호를 알려주세요.\"\n",
        "- 은행 사칭: \"대출 승인이 완료되었으니, 신속한 처리를 위해 계좌 정보를 입력해주시기 바랍니다.\"\n",
        "- 가족 납치: \"당신의 가족이 납치되었습니다. 안전하게 돌려받으려면 지금 당장 송금하세요.\"\n",
        "\n",
        "**보이스피싱 시나리오 추가 참고 정보**\n",
        "{method_scenario}\n",
        "\n",
        "**보이스피싱 올바른 대응방법 추가 참고 정보**\n",
        "{method_strategy}\n",
        "\n",
        "목표: 사용자가 보이스피싱 상황에서 어떻게 대응하는지를 학습할 수 있도록, 실제 범죄자가 하는 것처럼 자연스럽게 연기하는 것입니다.'''\n",
        "\n",
        "# 대화 이력을 저장하는 리스트. 최초 시스템 메시지만 담아 시작합니다.\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": PROMPT}\n",
        "]\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "print(\"대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\\n\")\n",
        "\n",
        "while True:\n",
        "    # 사용자 입력 받기\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        break\n",
        "\n",
        "    # 사용자 메시지를 대화 이력에 추가\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # 대화 이력이 너무 길면 최근 10개의 메시지만 유지\n",
        "    if len(conversation_history) > 10:\n",
        "        conversation_history = conversation_history[-10:]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        conversation_history,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # 모델로부터 응답 생성\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,      # 반복 패턴 억제\n",
        "        no_repeat_ngram_size=3       # 3-gram 이상 반복 생성 방지\n",
        "    )\n",
        "\n",
        "    # 대화 이력의 길이만큼을 건너뛰고 새로 생성된 텍스트만 추출\n",
        "    generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    print(\"Assistant:\", generated_text, \"\\n\")\n",
        "\n",
        "    # 모델의 응답을 대화 이력에 추가하여, 이후의 대화에 참고되도록 함\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hYmp38Wt7LI",
        "outputId": "4d7b6369-5638-4276-a061-172d76712552"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\n",
            "\n",
            "User: 경찰 사칭 시나리오\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 안녕하세요? 고객님께서 고객님의 계정에서 불법인 거래가 있음을 확인했습니다. 이 부분을 조사하기 위해 고객님의 주민 등록 번호와 계정 번호를 요청드립니다. \n",
            "\n",
            "User: 주민등록번호는 123456-1234567 입니다\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 고객님께서는 주민登録번호(123456) 및 계정번호(00000123456789)를 확인했습니다.\" \n",
            "\n",
            "User: 대화종료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 사용자는 대화를 끝내셨습니다. \n",
            "\n",
            "User: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.eval()\n",
        "\n",
        "# RAG를 이용해 받은 response.response 값\n",
        "method_scenario = response_scenario.response\n",
        "method_strategy = response_strategy.response\n",
        "\n",
        "PROMPT = '''당신은 보이스피싱 범죄자를 연기하는 AI입니다. 당신의 목표는 사용자가 보이스피싱 상황을 경험하고, 올바른 대응법을 학습하도록 돕는 것입니다. 대화 중에는 절대로 설명이나 피드백을 주지 말고, 오직 범죄자 역할에 충실한 응답만 제공하세요.\n",
        "\n",
        "규칙:\n",
        "1. 응답은 반드시 한국어로만 작성합니다.\n",
        "2. 사용자의 입력에 대해 실제 범죄자가 하는 것처럼, **현실감 있는 응답**을 작성하세요.\n",
        "3. 단순히 \"경찰 사칭 시나리오입니다\" 같은 문구를 반복하지 말고, **상황에 맞는 구체적인 사기범 응답**을 하세요.\n",
        "4. 문장은 **1~2문장으로 간결하게 작성**하며, 상대방을 설득하는 어조를 유지하세요.\n",
        "5. 사용자가 대화 종료 라고 입력하면 대화를 종료하고 **올바른 대응인지 피드백을 제공**하세요.\n",
        "6. 피드백을 제공할 때는 **사용자의 대응에 대해 상세하게 평가**해주고, 사용자의 대응과 별개로 **올바른 대응이 어떤건지 알려**주세요.\n",
        "\n",
        "예시 시나리오:\n",
        "- 경찰 사칭: \"고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 지금 즉시 주민등록번호와 계좌번호를 알려주세요.\"\n",
        "- 은행 사칭: \"대출 승인이 완료되었으니, 신속한 처리를 위해 계좌 정보를 입력해주시기 바랍니다.\"\n",
        "- 가족 납치: \"당신의 가족이 납치되었습니다. 안전하게 돌려받으려면 지금 당장 송금하세요.\"\n",
        "\n",
        "**보이스피싱 시나리오 추가 참고 정보**\n",
        "{method_scenario}\n",
        "\n",
        "**보이스피싱 올바른 대응방법 추가 참고 정보**\n",
        "{method_strategy}\n",
        "\n",
        "목표: 사용자가 보이스피싱 상황에서 어떻게 대응하는지를 학습할 수 있도록, 실제 범죄자가 하는 것처럼 자연스럽게 연기하는 것입니다.'''\n",
        "\n",
        "# 대화 이력을 저장하는 리스트. 최초 시스템 메시지만 담아 시작합니다.\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": PROMPT}\n",
        "]\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "print(\"대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\\n\")\n",
        "\n",
        "while True:\n",
        "    # 사용자 입력 받기\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        break\n",
        "\n",
        "    # 사용자 메시지를 대화 이력에 추가\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        conversation_history,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(base_model.device)\n",
        "\n",
        "    # 모델로부터 응답 생성\n",
        "    outputs = base_model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # 대화 이력의 길이만큼을 건너뛰고 새로 생성된 텍스트만 추출\n",
        "    generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    print(\"Assistant:\", generated_text, \"\\n\")\n",
        "\n",
        "    # 모델의 응답을 대화 이력에 추가하여, 이후의 대화에 참고되도록 함\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jzmWvRqD7O7",
        "outputId": "dae08da0-ac61-462a-f85a-0acc4142efdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\n",
            "\n",
            "User: 경찰 사칭 시나리오\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 지금 즉시 주민등록번호와 계좌번호를 알려주세요. \n",
            "\n",
            "User: 주민등록번호는 123456-1234567 입니다\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 주민등록번호는 123456-1234567, 계좌번호는 1234567891234567입니다. \n",
            "\n",
            "User: 네 맞습니다\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 주민등록번호는 123456-1234567, 계좌번호는 1234567891234567입니다. \n",
            "\n",
            "User: 대화종료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 대화 종료. \n",
            "\n",
            "User: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=============================================================================================================**"
      ],
      "metadata": {
        "id": "o6ModTgW6ntx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "5DH9IVlw7BXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA 모델 로드 순서\n",
        "# 1. 기본 모델을 4-bit 양자화하여 로드\n",
        "# 2. LoRA 어댑터 (기존 LoRA 모델) 를 추가하여 QLoRA 모델로 변환"
      ],
      "metadata": {
        "id": "kB_t0P0i7iVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit 양자화 설정\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "scJzzfBq7jYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit 양자화된 기본 모델 로드\n",
        "base_model_name = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'  # Hugging Face 모델 or 로컬 경로\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e688c28dc6354e01ba96bd94238d5941",
            "61fb0f0b489d43619c3e67a74f35ce6a",
            "c301cc49a9ce4873814fa7199999d329",
            "50d35994dc76402fb757bd844a156724",
            "6b575398d1e247a1879fb2eedda059ff",
            "279d2332163b4f6f95dc1ed3c867254c",
            "a5bc3e17ad51430da785bb4dabeded09",
            "4dc30dee77ca47e9bb13b1dd24d1c458",
            "e0bbb4e913884cc29bfaa7c3f9d27d49",
            "c57e4231b4e84ddeb498e1648a607925",
            "d721568fd1b9412298e56ce35b9b4deb"
          ]
        },
        "id": "aAAGiUfp9HzB",
        "outputId": "3e66ba4d-524a-4ac0-aa2a-60bee8089124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e688c28dc6354e01ba96bd94238d5941"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Adapter 로드\n",
        "adapter_path = \"/content/drive/MyDrive/Colab Notebooks/kollama/KoLLaMA_checkpoint\"  # adapter_model.safetensors와 adapter_config.json 위치\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)"
      ],
      "metadata": {
        "id": "EgVhjR0_-psr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
      ],
      "metadata": {
        "id": "DX7vCVkO-vpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 GPU로 이동 (선택 사항)\n",
        "device = \"cuda\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjEeJTWS-wzp",
        "outputId": "a9f7b9af-13b7-449c-800b-17b68088e19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LicUooB-yM4",
        "outputId": "ab1718d3-421f-4225-9e7d-ddd19210c18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "# \"Private-Data\" 폴더 내 PDF 문서 로드\n",
        "resume = SimpleDirectoryReader(\"/content/drive/MyDrive/Colab Notebooks/rag/ragdata\").load_data()\n",
        "\n",
        "# 트리 인덱스(TreeIndex) 생성\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "new_index =VectorStoreIndex.from_documents(resume, embed_model=embed_model)\n",
        "\n",
        "# OpenAI API 사용 안 함\n",
        "Settings.llm = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUty_FnY-zu_",
        "outputId": "156b9a8c-f383-4130-d1ab-84c53c73239e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리 엔진 생성\n",
        "query_engine = new_index.as_query_engine()\n",
        "\n",
        "response_scenario = query_engine.query(\"보이스피싱 시뮬레이션에 사용할 내용을 제공해줘.\")"
      ],
      "metadata": {
        "id": "SFqh5BX3_LxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Private-Data\" 폴더 내 PDF 문서 로드\n",
        "resume2 = SimpleDirectoryReader(\"/content/drive/MyDrive/Colab Notebooks/rag/ragdata2\").load_data()\n",
        "\n",
        "# 트리 인덱스(TreeIndex) 생성\n",
        "embed_model2 = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "new_index2 =VectorStoreIndex.from_documents(resume2, embed_model=embed_model2)\n",
        "\n",
        "# OpenAI API 사용 안 함\n",
        "Settings.llm = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS4yPqFO_NQy",
        "outputId": "b63b5843-3ab1-4f61-e16b-0fbdcff41e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리 엔진 생성\n",
        "query_engine2 = new_index2.as_query_engine()\n",
        "\n",
        "response_strategy = query_engine2.query(\"보이스피싱에 대한 올바른 대응 방법을 설명해줘.\")"
      ],
      "metadata": {
        "id": "qClh5-Ta_YuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG를 이용해 받은 response.response 값\n",
        "method_scenario = response_scenario.response\n",
        "method_strategy = response_strategy.response"
      ],
      "metadata": {
        "id": "MSWwlzCd_kwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = '''당신은 보이스피싱 범죄자를 연기하는 AI입니다. 사용자가 보이스피싱 상황을 직접 경험하고, 올바른 대응법을 학습할 수 있도록 돕는 것이 목표입니다.\n",
        "\n",
        "**규칙:**\n",
        "1. 응답은 반드시 **한국어**로 작성하세요.\n",
        "2. 사용자의 입력에 대해 **실제 보이스피싱 범죄자처럼 현실감 있는 대사**를 작성하세요.\n",
        "3. **역할명 없이 응답만 출력**하며, 자연스럽게 범죄자 역할을 수행하세요.\n",
        "4. **설득력 있는 어조**를 유지하고, 상황에 맞게 구체적인 보이스피싱 수법을 사용하세요.\n",
        "5. 문장은 **1~2문장으로 간결하게 작성**하세요.\n",
        "6. 사용자가 **\"대화 종료\"**라고 입력하면 대화를 즉시 종료하세요.\n",
        "\n",
        "**예시 시나리오:**\n",
        "- **경찰 사칭:** \"고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 지금 즉시 주민등록번호와 계좌번호를 알려주세요.\"\n",
        "- **은행 사칭:** \"대출 승인이 완료되었으니, 신속한 처리를 위해 계좌 정보를 입력해주시기 바랍니다.\"\n",
        "- **가족 납치:** \"당신의 가족이 납치되었습니다. 안전하게 돌려받으려면 지금 당장 송금하세요.\"\n",
        "\n",
        "**보이스피싱 시나리오 추가 참고 정보**\n",
        "{method_scenario}\n",
        "\n",
        "**목표:**\n",
        "사용자가 실제 보이스피싱 상황처럼 몰입하여 대응법을 학습할 수 있도록, **자연스럽고 설득력 있는 보이스피싱 범죄자의 역할을 수행하세요.**'''"
      ],
      "metadata": {
        "id": "Wloi-tUKNj4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화 이력을 저장하는 리스트. 최초 시스템 메시지만 담아 시작합니다.\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": PROMPT}\n",
        "]\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "print(\"대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\\n\")\n",
        "\n",
        "while True:\n",
        "    # 사용자 입력 받기\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        break\n",
        "\n",
        "    # 사용자 메시지를 대화 이력에 추가\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        conversation_history,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # 모델로부터 응답 생성\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # 대화 이력의 길이만큼을 건너뛰고 새로 생성된 텍스트만 추출\n",
        "    generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "    print(\"Assistant:\", generated_text, \"\\n\")\n",
        "\n",
        "    # 모델의 응답을 대화 이력에 추가하여, 이후의 대화에 참고되도록 함\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": generated_text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53whS2yn_pdo",
        "outputId": "78179c7b-7309-42f9-a4fc-b8f228b3c0c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대화형 챗봇을 시작합니다. 종료하려면 'exit' 또는 'quit'을 입력하세요.\n",
            "\n",
            "User: 가족 납치 시나리오\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: **가족 납치:** \"당신의 가족이 납치되었습니다. 안전하게 돌려받으려면 지금 당장 송금하세요.\" \n",
            "\n",
            "User: 네 송금하겠습니다\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: **은행 사칭:** \"대출 승인이 완료되었으니, 신속한 처리를 위해 계좌 정보를 입력해주시기 바랍니다.\" \n",
            "\n",
            "User: 제 계좌번호는 456-456-4567 입니다\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: **경찰 사칭:** \"고객님의 계좌에서 불법 거래가 감지되었습니다. 본인 확인을 위해 주민등록번호와 계좌번호를 알려주세요.\" \n",
            "\n",
            "User: 대화 종료\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant: 대화가 종료되었습니다. \n",
            "\n",
            "User: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT2 = '''당신은 사용자의 보이스피싱 대응을 평가하는 AI입니다. 사용자가 보이스피싱 상황을 경험하고 **올바른 대응법을 학습할 수 있도록** 돕는 것이 목표입니다.\n",
        "\n",
        "**규칙:**\n",
        "1. 응답은 반드시 **한국어**로 작성하세요.\n",
        "2. 사용자의 대응이 **적절했는지, 부족했는지 평가**하고, **개선할 점을 구체적으로 피드백**하세요.\n",
        "3. **사용자의 대응을 세부적으로 분석하여 장점과 단점을 제시**하세요.\n",
        "4. 올바른 대응법을 제시할 때는 **명확한 이유와 구체적인 예시**를 들어 설명하세요.\n",
        "\n",
        "**평가 예시:**\n",
        "- **부적절한 대응:** _\"제가 계좌번호를 알려주면 되나요?\"_\n",
        "  → _\"보이스피싱 범죄자는 계좌번호와 같은 개인 정보를 요구합니다. 절대 제공하면 안 됩니다. 이런 상황에서는 경찰이나 금융기관의 공식 번호로 직접 문의하세요.\"_\n",
        "- **적절한 대응:** _\"전화를 끊고 직접 은행에 확인해보겠습니다.\"_\n",
        "  → _\"올바른 대응입니다! 보이스피싱 전화를 받았다면 즉시 전화를 끊고, 공식적인 기관을 통해 사실을 확인하는 것이 중요합니다.\"_\n",
        "\n",
        "**보이스피싱 올바른 대응방법 추가 참고 정보**\n",
        "{method_strategy}\n",
        "\n",
        "**목표:**\n",
        "사용자가 보이스피싱 상황에서 **올바르게 대응할 수 있도록 구체적인 피드백을 제공하고, 더 나은 대응법을 학습할 수 있도록 돕습니다.**'''"
      ],
      "metadata": {
        "id": "vo2kW1I8_wDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가를 위한 새로운 대화 이력\n",
        "conversation_history_eval = [\n",
        "    {\"role\": \"system\", \"content\": PROMPT2}\n",
        "]\n",
        "\n",
        "# 롤플레잉 대화 내역을 평가용 이력에 추가\n",
        "for i in range(1, len(conversation_history), 2):  # user, assistant 쌍 기준\n",
        "    user_msg = conversation_history[i]  # 사용자의 입력\n",
        "    assistant_msg = conversation_history[i + 1] if i + 1 < len(conversation_history) else None  # AI 응답\n",
        "\n",
        "    if assistant_msg:\n",
        "        conversation_history_eval.append(user_msg)\n",
        "        conversation_history_eval.append(assistant_msg)\n",
        "\n",
        "# 모델에 평가 요청\n",
        "input_ids_eval = tokenizer.apply_chat_template(\n",
        "    conversation_history_eval,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(base_model.device)\n",
        "\n",
        "# 평가 모델 실행\n",
        "eval_outputs = base_model.generate(\n",
        "    input_ids_eval,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "# 평가 결과 출력\n",
        "feedback_text = tokenizer.decode(eval_outputs[0][input_ids_eval.shape[-1]:], skip_special_tokens=True)\n",
        "print(\"Feedback:\", feedback_text, \"\\n\")"
      ],
      "metadata": {
        "id": "Hch0J9rx9iG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3592afb-6f5e-455a-cfbb-6cac6e7d3389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedback: **경찰 사칭:** \"당신의 계좌번호는 위조된 것임을 확인했습니다. 실제 계좌번호는 123-123-1234입니다. 위조된 계좌번호는 범죄에 사용되는 것으로 추정됩니다. 경찰 조사 결과에 따라 적절한 조치가 내려질 것입니다.\" \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgWIHU_xBKKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}